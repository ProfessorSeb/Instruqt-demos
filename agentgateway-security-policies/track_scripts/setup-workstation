#!/bin/bash
set -euxo pipefail

# Wait for the Instruqt bootstrap to complete
until [ -f /opt/instruqt/bootstrap/host-bootstrap-completed ]; do
  echo "Waiting for Instruqt bootstrap..."
  sleep 2
done

echo "=== Installing k3s ==="
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable=traefik" sh -

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
until kubectl get nodes 2>/dev/null | grep -q ' Ready'; do
  echo "Waiting for k3s node to be Ready..."
  sleep 3
done

echo "=== Configuring shell environment ==="
cat >> /root/.bashrc << 'BASHEOF'
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
alias k=kubectl
source <(kubectl completion bash)
complete -o default -F __start_kubectl k
BASHEOF

echo "=== Installing tools ==="
apt-get update -qq && apt-get install -y -qq jq unzip bash-completion
curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

echo "=== Installing Gateway API CRDs ==="
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/standard-install.yaml

echo "=== Installing AgentGateway OSS via Helm ==="
helm repo add agentgateway https://solo-io.github.io/agentgateway
helm repo update

helm install agentgateway agentgateway/agentgateway \
  --namespace agentgateway-system \
  --create-namespace \
  --version 2.1.0 \
  --wait --timeout 120s

kubectl -n agentgateway-system rollout status deployment/agentgateway --timeout=120s

echo "=== Deploying mock LLM backend ==="
cat <<'K8SEOF' | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: mock-llm-server
  namespace: default
data:
  server.py: |
    #!/usr/bin/env python3
    import json, http.server, socketserver

    class Handler(http.server.BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path == "/health":
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(b'{"status":"ok"}')
            else:
                self.send_response(404)
                self.end_headers()

        def do_POST(self):
            length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(length).decode('utf-8')
            try:
                req = json.loads(body)
            except:
                req = {"messages": [{"role": "user", "content": body}]}

            user_msg = "Hello!"
            if "messages" in req:
                for msg in reversed(req["messages"]):
                    if msg.get("role") == "user":
                        user_msg = msg.get("content", "Hello!")
                        break

            response = {
                "id": "mock-001",
                "object": "chat.completions",
                "model": req.get("model", "mock-gpt-4"),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": f"I received your message. You said: {user_msg}\n\nHere is my response."}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 20, "completion_tokens": 30, "total_tokens": 50}
            }

            resp = json.dumps(response).encode()
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Content-Length', str(len(resp)))
            self.end_headers()
            self.wfile.write(resp)

        def log_message(self, fmt, *args): pass

    socketserver.TCPServer(("", 8080), Handler).serve_forever()
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mock-llm
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mock-llm
  template:
    metadata:
      labels:
        app: mock-llm
    spec:
      containers:
        - name: mock-llm
          image: python:3.11-slim
          command: ["python3", "/app/server.py"]
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: server
              mountPath: /app
      volumes:
        - name: server
          configMap:
            name: mock-llm-server
---
apiVersion: v1
kind: Service
metadata:
  name: mock-llm
  namespace: default
spec:
  selector:
    app: mock-llm
  ports:
    - port: 8080
      targetPort: 8080
K8SEOF

kubectl rollout status deployment/mock-llm --timeout=120s

mkdir -p /root/policies

echo "=== Track setup complete ==="
