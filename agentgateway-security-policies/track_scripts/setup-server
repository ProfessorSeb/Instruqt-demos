#!/bin/bash

# Wait for instruqt bootstrap
until [ -f /opt/instruqt/bootstrap/host-bootstrap-completed ]; do
  sleep 1
done

set -euxo pipefail

set-workdir /root

# Shell conveniences
cat >> ~/.bashrc << 'EOF'
alias k=kubectl
complete -F __start_kubectl k
source /etc/bash_completion
EOF

# Start k3d cluster (pre-installed in image)
if command -v k3d >/dev/null 2>&1; then
  k3d cluster start my-k8s-cluster 2>/dev/null || \
  k3d cluster create my-k8s-cluster --wait
fi

kubectl wait --for=condition=Ready nodes --all --timeout=120s

echo "=== Installing Gateway API CRDs ==="
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/standard-install.yaml

echo "=== Installing AgentGateway CRDs ==="
helm install agentgateway-crds oci://ghcr.io/kgateway-dev/charts/agentgateway-crds \
  --namespace agentgateway-system \
  --create-namespace \
  --version v2.2.0 \
  --wait

echo "=== Installing AgentGateway Control Plane ==="
helm install agentgateway oci://ghcr.io/kgateway-dev/charts/agentgateway \
  --namespace agentgateway-system \
  --version v2.2.0 \
  --wait --timeout 120s

kubectl -n agentgateway-system rollout status deployment/agentgateway --timeout=120s

echo "=== Deploying mock LLM backend ==="
cat <<'K8SEOF' | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: mock-llm-server
  namespace: default
data:
  server.py: |
    #!/usr/bin/env python3
    import json, http.server, socketserver

    class Handler(http.server.BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path == "/health":
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(b'{"status":"ok"}')
            else:
                self.send_response(404)
                self.end_headers()

        def do_POST(self):
            length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(length).decode('utf-8')
            try:
                req = json.loads(body)
            except:
                req = {"messages": [{"role": "user", "content": body}]}

            user_msg = "Hello!"
            if "messages" in req:
                for msg in reversed(req["messages"]):
                    if msg.get("role") == "user":
                        user_msg = msg.get("content", "Hello!")
                        break

            response = {
                "id": "mock-001",
                "object": "chat.completions",
                "model": req.get("model", "mock-gpt-4"),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": f"I received your message. You said: {user_msg}\n\nHere is my response."}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 20, "completion_tokens": 30, "total_tokens": 50}
            }

            resp = json.dumps(response).encode()
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Content-Length', str(len(resp)))
            self.end_headers()
            self.wfile.write(resp)

        def log_message(self, fmt, *args): pass

    socketserver.TCPServer(("", 8080), Handler).serve_forever()
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mock-llm
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mock-llm
  template:
    metadata:
      labels:
        app: mock-llm
    spec:
      containers:
        - name: mock-llm
          image: python:3.11-slim
          command: ["python3", "/app/server.py"]
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: server
              mountPath: /app
      volumes:
        - name: server
          configMap:
            name: mock-llm-server
---
apiVersion: v1
kind: Service
metadata:
  name: mock-llm
  namespace: default
spec:
  selector:
    app: mock-llm
  ports:
    - port: 8080
      targetPort: 8080
K8SEOF

kubectl rollout status deployment/mock-llm --timeout=120s

mkdir -p /root/policies

# Create OpenAI API key secret for use in challenges
if [ -n "$OPENAI_API_KEY" ]; then
  echo "export OPENAI_API_KEY=${OPENAI_API_KEY}" >> ~/.bashrc
  kubectl create secret generic openai-secret -n agentgateway-system \
    --from-literal="Authorization=Bearer $OPENAI_API_KEY" \
    --dry-run=client -oyaml | kubectl apply -f -
  echo "=== OpenAI API key secret created ==="
else
  echo "=== WARNING: OPENAI_API_KEY not set, using placeholder ==="
  kubectl create secret generic openai-secret -n agentgateway-system \
    --from-literal="Authorization=Bearer sk-demo-placeholder-key" \
    --dry-run=client -oyaml | kubectl apply -f -
fi

echo "=== Track setup complete ==="
